{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit NLP- How to differentiate between AmItheAsshole and Legal Advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(csv):\n",
    "    dataframe = pd.read_csv(csv)\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    #remove null values\n",
    "    dataframe = dataframe[dataframe['selftext'].notnull()]\n",
    "    \n",
    "    #drop \"removed\" posts\n",
    "    dataframe = dataframe[dataframe['selftext'] != '[removed]']\n",
    "    # combine post and title text\n",
    "    dataframe['fulltext'] = dataframe['selftext'] + dataframe['title']\n",
    "    # clean data\n",
    "    dataframe['fulltext'] =[str(words).lower() for words in dataframe['fulltext']]\n",
    "    dataframe['fulltext'] =[str(words).replace(\"'\", \"\").replace(\".\",\"\").replace(\"(\", \"\").replace(\")\", \"\") for words in dataframe['fulltext']]\n",
    "    dataframe['fulltext'] =[str(words).replace(\":\", \"\").replace(\"*\",\"\").replace('\"', \"\") for words in dataframe['fulltext']]\n",
    "    \n",
    "    #lemmatize data: (with help from https://stackoverflow.com/questions/47557563/lemmatization-of-all-pandas-cells)\n",
    "    \n",
    "    dataframe['fulltext'] = [w_tokenizer.tokenize(string) for string in dataframe['fulltext']]\n",
    "    for words in dataframe['fulltext']:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    dataframe['fulltext'] = [' '.join(words) for words in dataframe['fulltext']]\n",
    "    \n",
    "    #return dataframe\n",
    "    return dataframe\n",
    "    #convert all text to lowercase\n",
    "    # remove punctuation\n",
    "    #stem or lemmatize each word of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# Run both subreddits of data through the set and return  \n",
    "aita = preprocessing('./data/AITA_raw.csv')\n",
    "legal_advice = preprocessing('./data/legal_advice_raw.csv')\n",
    "combined = legal_advice.append(aita)\n",
    "# remove AITA to make predictions more accurate\n",
    "#(aka remove a fake word that will be in every post of one subreddit)\n",
    "combined['fulltext'] =[str(words).replace(\"aita\", \"\") for words in combined['fulltext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the Y into a 0/1 column!\n",
    "combined['subreddit'] = combined['subreddit'].map({'legaladvice': 1, \"AmItheAsshole\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modified from https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "\n",
    "def get_top_n_words(corpus, n = 20):\n",
    "    bag_of_words = cvec.fit_transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in cvec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "aita['fulltext']\n",
    "la_words = get_top_n_words(legal_advice['fulltext'])\n",
    "aita_words = get_top_n_words(aita['fulltext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above code, I looked at the most frequent words in each subreddit. It was interesting how many more words there were, period, in AITA vs Legal Advice, as well as the fact that some casual speaking mannerisms (using Just, Like, and Really, for example) were much common in one thread than the other, suggesting that n-grams increases would better my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('just', 3047),\n",
       " ('im', 2602),\n",
       " ('time', 2531),\n",
       " ('know', 2281),\n",
       " ('said', 2093),\n",
       " ('work', 2067),\n",
       " ('told', 2062),\n",
       " ('amp', 2058),\n",
       " ('like', 1971),\n",
       " ('car', 1839),\n",
       " ('pay', 1761),\n",
       " ('x200b', 1712),\n",
       " ('got', 1682),\n",
       " ('company', 1657),\n",
       " ('want', 1640),\n",
       " ('going', 1540),\n",
       " ('dont', 1516),\n",
       " ('did', 1507),\n",
       " ('house', 1420),\n",
       " ('legal', 1420)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('just', 6432),\n",
       " ('like', 5509),\n",
       " ('im', 4286),\n",
       " ('time', 4025),\n",
       " ('aita', 3979),\n",
       " ('said', 3895),\n",
       " ('told', 3710),\n",
       " ('really', 3378),\n",
       " ('friend', 3259),\n",
       " ('know', 3164),\n",
       " ('want', 3145),\n",
       " ('friends', 3083),\n",
       " ('dont', 2825),\n",
       " ('got', 2741),\n",
       " ('feel', 2570),\n",
       " ('going', 2442),\n",
       " ('didnt', 2352),\n",
       " ('day', 2090),\n",
       " ('people', 2078),\n",
       " ('work', 2023)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establish variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined['fulltext']\n",
    "y = combined['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logestic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline logistic regression, with no parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9998346013893483"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec_X_train = cvec.fit_transform(X_train)\n",
    "cvec_X_test = cvec.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(cvec_X_train, y_train)\n",
    "lr.score(cvec_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9603174603174603"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(cvec_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran several dozen variations of the pipeline and gridsearch outlined below in order ot find my model's best parameters. For everyone's sake, I've currently left these not running, but I tried the following parameters as in attempts to improve my baseline logistic regression score:\n",
    "\n",
    "For CVEC:\n",
    "\n",
    "- max features ranging from 3,000 to none (447,000 with ngram range (1,2))\n",
    "- n-gram ranges (1,1), (1,2) and (1,3) (which did not increase my score enough to justify the computational time.)\n",
    "- stopwords 'english' and none (none performed better every time)\n",
    "\n",
    "For Logistic Regression:\n",
    "\n",
    "- Penalties l1 and l2 (lasso consistently outperformed ridge, while ridge rarely improved upon the baseline)\n",
    "- Alphas ranging from 1 to 1000 (the two you see below were the best performing in logspace(1, 5, 10)\n",
    "\n",
    "Additionally, I attempted to run TF-IDF instead of Count Vectorizer, expecting it would perform better because one of my subreddits (AITA) was just wordier than the other subreddits, and I thought it might better account for that fact, but it performed about .5% worse on average.\n",
    "\n",
    "No variations on my logistic regression model were able to reduce the amount my model was overfit to less than #% (100% vs 97%.) All efforts I made to lower my training score impacted my testing score quite negatively--evening them out wasn't possible. I think further data collection would be the next step in improving this logistic regression model beyond what I have currently tried."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "         ])\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [None, 250_000, 300_000, 350_000,],\n",
    "    'cvec__ngram_range': [(1, 2)],\n",
    "    'lr__penalty': ['l1'],\n",
    "    'lr__C': [46.4159, 166.81]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "gs= GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_params_\n",
    "\n",
    "gs.best_score_\n",
    "\n",
    "gs.score(X_train, y_train)\n",
    "\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Model:\n",
    "cvec = CountVectorizer(max_features = None, ngram_range=(1,2))\n",
    "cvec_X_train = cvec.fit_transform(X_train)\n",
    "cvec_X_test = cvec.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', C = 166.81)\n",
    "lr.fit(cvec_X_train, y_train)\n",
    "lr.score(cvec_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672619047619048"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(cvec_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier I tested never performed better than the logistic regression model. It was less overfit, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('cvec', CountVectorizer()),\n",
    "        ('nb', MultinomialNB(alpha=0))\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {'cvec__max_features':[None, 200000, 250000],\n",
    "               'cvec__ngram_range': [(1, 2)],\n",
    "               'cvec__stop_words': [None]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs= GridSearchCV(pipe, param_grid=pipe_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cvec__max_features': [None, 200000, 250000], 'cvec__ngram_range': [(1, 2)], 'cvec__stop_words': [None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 250000,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9199470724445915"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996692027786966"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9255952380952381"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried a Random Forrest model to see if I could manipulate its parameters enough to reduce my variance in my model, but it actually included significantly more variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec_X_train = cvec.fit_transform(X_train)\n",
    "cvec_X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(cvec_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9960304333443599"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(cvec_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8526785714285714"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(cvec_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('cvec', CountVectorizer(ngram_range=(1, 2), max_features=300_000)),\n",
    "        ('rf',RandomForestClassifier())\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_params = {'cvec__max_features':[None, 15000, 20000],\n",
    "#                'cvec__ngram_range': [(1, 2), (1,3)],\n",
    "#                'cvec__stop_words': [None, 'english']\n",
    "#               }\n",
    "\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [200, 400, 600],\n",
    "    'rf__max_depth': [None, 5000],\n",
    "    'rf__min_samples_split': [5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs= GridSearchCV(pipe, param_grid=rf_params, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        str...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'rf__n_estimators': [200, 400, 600], 'rf__max_depth': [None, 5000], 'rf__min_samples_split': [5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 5000, 'rf__min_samples_split': 5, 'rf__n_estimators': 400}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191468253968254"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Conclusions:\n",
    "\n",
    "The best model, by far, for this data set is a logistic regression. With 97% accuracy, and only 3% variance between my training and testing data, my final model performed wonderfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze my model, and understand why it made the decisions it made, I first examined my confusion matrix, and then wrote several functions to help me look at the most prominent features of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=166.81, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(ngram_range=(1,2))\n",
    "cvec_X_train = cvec.fit_transform(X_train)\n",
    "cvec_X_test = cvec.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', C = 166.81)\n",
    "lr.fit(cvec_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(cvec_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 897\n",
      "False Positives: 39\n",
      "False Negatives: 22\n",
      "True Positives: 1058\n"
     ]
    }
   ],
   "source": [
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_chart = pd.DataFrame()\n",
    "predict_chart['text'] = X_test\n",
    "predict_chart['true_values'] = y_test\n",
    "predict_chart['predicted_values'] = predictions\n",
    "predict_chart.reset_index(drop=True, inplace=True)\n",
    "false_predictions = predict_chart[predict_chart['predicted_values'] != predict_chart['true_values']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced below, the number of features involved in this dataset is much smaller than the number of max features that are optimized using gridsearch! This is because many of the strong predictors are not used many times, but do impact the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2458, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = pd.DataFrame(columns=['coefficient', 'variable'])\n",
    "coefficients['coefficient'] = lr.coef_.tolist()[0]\n",
    "coefficients['variable'] = cvec.vocabulary_.items()\n",
    "non_zero_coefficients = coefficients[coefficients['coefficient'] != 0]\n",
    "non_zero_coefficients.reset_index(drop=True, inplace=True)\n",
    "non_zero_coefficients.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
